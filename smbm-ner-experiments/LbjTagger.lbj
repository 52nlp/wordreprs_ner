package LbjFeatures;

import java.util.*;
import LbjTagger.NEWord;
import ExpressiveFeatures.BrownClusters;
import ExpressiveFeatures.Gazzetteers;
import ExpressiveFeatures.WordEmbeddings;
import ExpressiveFeatures.HmmEmbeddings;
import LbjTagger.ParametersForLbjCode;
import StringStatisticsUtils.*;


//---------------- CLASSIFIER LEVEL 1 -------------------


discrete% additionalFeaturesDiscreteNonConjunctive(NEWord word) <-
{
	for(int fid=0;fid<word.generatedDiscreteFeaturesNonConjunctive.size();fid++){
		NEWord.DiscreteFeature feature=word.generatedDiscreteFeaturesNonConjunctive.elementAt(fid);
		if(!feature.useWithinTokenWindow)
			sense feature.featureGroupName : feature.featureValue;
	}
	int i;
  	NEWord w = word, last = word;
  	for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
  	for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;

  	for (; w != last; w = (NEWord) w.next){
		for(int fid=0;fid<w.generatedDiscreteFeaturesNonConjunctive.size();fid++){
			NEWord.DiscreteFeature feature=w.generatedDiscreteFeaturesNonConjunctive.elementAt(fid);
			if(feature.useWithinTokenWindow)
				sense "pos"+i+"group"+feature.featureGroupName : feature.featureValue;
		}
  		i++;
  	}
}

//
// these will be used by themselves AND with conjunctions. particularly with the previous predictions
//
discrete% additionalFeaturesDiscreteConjunctive(NEWord word) <-
{
	for(int fid=0;fid<word.generatedDiscreteFeaturesConjunctive.size();fid++){
		NEWord.DiscreteFeature feature=word.generatedDiscreteFeaturesConjunctive.elementAt(fid);
		if(!feature.useWithinTokenWindow)
			sense feature.featureGroupName : feature.featureValue;
	}
	int i;
  	NEWord w = word, last = word;
  	for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
  	for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;

  	for (; w != last; w = (NEWord) w.next){
		for(int fid=0;fid<w.generatedDiscreteFeaturesConjunctive.size();fid++){
			NEWord.DiscreteFeature feature=w.generatedDiscreteFeaturesConjunctive.elementAt(fid);
			if(feature.useWithinTokenWindow)
				sense "pos"+i+"group"+feature.featureGroupName : feature.featureValue;
		}
  		i++;
  	}
}



real% additionalFeaturesRealNonConjunctive(NEWord word) <-
{
	for(int fid=0;fid<word.generatedRealFeaturesNonConjunctive.size();fid++){
		NEWord.RealFeature feature=word.generatedRealFeaturesNonConjunctive.elementAt(fid);
		if(!feature.useWithinTokenWindow)
			sense feature.featureGroupName : feature.featureValue;
	}
	int i;
  	NEWord w = word, last = word;
  	for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
  	for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;

  	for (; w != last; w = (NEWord) w.next){
		for(int fid=0;fid<w.generatedRealFeaturesNonConjunctive.size();fid++){
			NEWord.RealFeature feature=w.generatedRealFeaturesNonConjunctive.elementAt(fid);
			if(feature.useWithinTokenWindow)
				sense "pos"+i+"group"+feature.featureGroupName : feature.featureValue;
		}
  		i++;
  	}
}



//
// these will be used by themselves AND with conjunctions. particularly with the previous predictions
//
real% additionalFeaturesRealConjunctive(NEWord word) <-
{
	for(int fid=0;fid<word.generatedRealFeaturesConjunctive.size();fid++){
		NEWord.RealFeature feature=word.generatedRealFeaturesConjunctive.elementAt(fid);
		if(!feature.useWithinTokenWindow)
			sense feature.featureGroupName : feature.featureValue;
	}
	int i;
  	NEWord w = word, last = word;
  	for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
  	for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;

  	for (; w != last; w = (NEWord) w.next){
  		for(int fid=0;fid<w.generatedRealFeaturesConjunctive.size();fid++){
			NEWord.RealFeature feature=w.generatedRealFeaturesConjunctive.elementAt(fid);
			if(feature.useWithinTokenWindow)
				sense "pos"+i+"group"+feature.featureGroupName : feature.featureValue;
		}
  		i++;
  	}
}


discrete% GazetteersFeatures(NEWord word) <-
{
	if(ParametersForLbjCode.featuresToUse.containsKey("GazetteersFeatures"))
	{ 
 		int i=0;
   		NEWord w = word, last = (NEWord)word.next;
  
   		for (i = 0; i < 2 && last != null; ++i) last = (NEWord) last.next;
   		for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;
  
 		do 
   		{
	 		if(w.gazetteers!=null)
		 		for(int j=0;j<w.gazetteers.size();j++)
					sense i: w.gazetteers.elementAt(j);
	 		i++;
	 		w = (NEWord) w.next;
   		}while(w != last);
 	}
}


real% WordEmbeddingFeatures(NEWord word) <-
{
  	if(ParametersForLbjCode.featuresToUse.containsKey("WordEmbeddings"))
	{ 
  		int i;
  		NEWord w = word, last = word;
  		for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
  		for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;

  		for (; w != last; w = (NEWord) w.next) {
  			double[] embedding=WordEmbeddings.getEmbedding(w);
			if(embedding!=null)
				for(int dim=0;dim<embedding.length;dim++)
					sense "place"+i+"dim"+dim : embedding[dim];
			i++;
		}
	}
}

real% HmmEmbeddingFeatures(NEWord word) <-
{
  	if(ParametersForLbjCode.featuresToUse.containsKey("HmmEmbeddings"))
	{ 
  		int i;
  		NEWord w = word, last = word;
  		for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
  		for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;

  		for (; w != last; w = (NEWord) w.next) {
  			double[] embedding=HmmEmbeddings.getEmbedding(w);
			if(embedding!=null){
                //System.out.println("The embedding for the token "+w.form+" is: [ "); 
                //double sum=0; 
				for(int dim=0;dim<embedding.length;dim++){
					sense "place"+i+"dim"+dim : embedding[dim];
	                //System.out.print(" "+embedding[dim]); 
                    //sum+=embedding[dim]; 
				}
                //System.out.println(" ] The dimension is: "+embedding.length+" ;  the embedding norm is: "+sum); 
			}
			else{
				//System.out.println(" Warning: null embedding for the word: "+w.form+" hmm embedding key: "+ w.sentenceId+"_"+w.tokenId);
			}
			i++;
		}
	}
}


discrete% Forms(NEWord word) <-
{
  	if(ParametersForLbjCode.featuresToUse.containsKey("Forms"))
	{ 
  		int i;
  		NEWord w = word, last = word;
  		for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
  		for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;

		int startIndex=i;
		NEWord startWord=w;
  		for (; w != last; w = (NEWord) w.next) sense i++ : w.form;
  		i=startIndex;
  		w=startWord;
  		for (; w != last; w = (NEWord) w.next){
			sense i : MyString.normalizeDigitsForFeatureExtraction(w.form);
			i++;
		}
	}
}


// Problem 1
discrete% BrownClusterPaths(NEWord word) <-
{
  	if(ParametersForLbjCode.featuresToUse.containsKey("BrownClusterPaths"))
	{ 
  		int i;
  		NEWord w = word, last = word;
  		for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
  		for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;

  		for (; w != last; w = (NEWord) w.next){
  			String[] paths=BrownClusters.getPrefixes(w);
  			for(int j=0;j<paths.length;j++)
  				sense i : paths[j];
  			i++;
  		}
	}
}


// Problem 1
discrete% FormParts(NEWord word) <-
{
  	if(ParametersForLbjCode.featuresToUse.containsKey("Forms")&&
  		ParametersForLbjCode.tokenizationScheme.equals(ParametersForLbjCode.TokenizationScheme.DualTokenizationScheme))
	{ 
		sense "0" : word.form;
		int i=-1;
		int count=-1;
		NEWord w = (NEWord)word.previous;
		while(w!=null&&i>=-2){
			String[] lastParts= w.parts;
			for(int j=0;j<lastParts.length;j++)
			{
				sense count:  MyString.normalizeDigitsForFeatureExtraction(lastParts[j]);
				count--;
			}
			w = (NEWord)w.previous;
			i--;
		}
		i=1;
		count=1;
		w = (NEWord)word.next;
		while(w!=null&&i<=2){
			String[] lastParts= w.parts;
			for(int j=0;j<lastParts.length;j++)
			{
				sense count:  MyString.normalizeDigitsForFeatureExtraction(lastParts[j]);
				count++;
			}
			w = (NEWord)w.next;
			i++;
		}
  	}
}


// Feature set i
discrete{false, true}% Capitalization(NEWord word) <-
{
	if(ParametersForLbjCode.featuresToUse.containsKey("Capitalization"))
	{ 
	 	int i;
  		NEWord w = word, last = word;
  		for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
  		for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;

  		for (; w != last; w = (NEWord) w.next) sense i++ : w.capitalized;
  	}
}

// Feature set ii
discrete{false, true}% WordTypeInformation(NEWord word) <-
{
	if(ParametersForLbjCode.featuresToUse.containsKey("WordTypeInformation"))
	{ 
	  int i;
	  NEWord w = word, last = word;
	  for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
	  for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;

	  for (; w != last; w = (NEWord) w.next, ++i)
	  {
	    boolean allCapitalized = true, allDigits = true, allNonLetters = true;
	
	    for (int j = 0; j < w.form.length(); ++j)
	    {
	      allCapitalized &= Character.isUpperCase(w.form.charAt(j));
	      allDigits &= Character.isDigit(w.form.charAt(j));
	      allNonLetters &= !Character.isLetter(w.form.charAt(j));
	    }

	    sense "c" + i : allCapitalized;
	    sense "d" + i : allDigits;
	    sense "p" + i : allNonLetters;
  	  }
  	}
}

// Feature set iii
discrete% Affixes(NEWord word) <-
{
	if(ParametersForLbjCode.featuresToUse.containsKey("Affixes"))
	{ 
  		int N = word.form.length();
	  	for (int i = 3; i <= 4; ++i)
    		if (word.form.length() > i) sense "p|" : word.form.substring(0, i);
  		for (int i = 1; i <= 4; ++i)
    		if (word.form.length() > i) sense "s|" : word.form.substring(N - i);
		
		if(ParametersForLbjCode.tokenizationScheme.equals(ParametersForLbjCode.TokenizationScheme.DualTokenizationScheme))	
			for(int i=0;i<word.parts.length;i++)
				sense "part"+i : word.parts[i];

	}
}


discrete NELabel(NEWord word) <- { return word.neLabel; }

real% nonLocalFeatures(NEWord word) <-
{
	//no need to check which features are active here- if 
	//nonlocal features are not used, they will not be generated! 
	String[] feats=word.getAllNonlocalFeatures();
	for(int i=0;i<feats.length;i++)
		sense feats[i]: word.getNonLocFeatCount(feats[i]);
}


// Feature set iv
discrete% PreviousTag1Level1(NEWord word) <-
{
	if(ParametersForLbjCode.featuresToUse.containsKey("PreviousTag1"))
	{ 
	  	int i;
	  	NEWord w = word;
	  	if(w.previous!=null)
	  	{
			if (NETaggerLevel1.isTraining/*&&ParametersForLbjCode.trainingIteration==0*/) 
	    		sense "-1" : ((NEWord)w.previous).neLabel;
	    	else
	    		sense "-1" : ((NEWord)w.previous).neTypeLevel1;
      	}
    }
}

// Feature set iv
discrete% PreviousTag2Level1(NEWord word) <-
{
	if(ParametersForLbjCode.featuresToUse.containsKey("PreviousTag2"))
	{ 
	  int i;
	  NEWord w = word;
	  if(w.previous!=null)
	  {
		if(((NEWord)w.previous).previous!=null)
	  	{
			if (NETaggerLevel1.isTraining/*&&ParametersForLbjCode.trainingIteration==0*/) 
		   		sense "-2" : ((NEWord)((NEWord)w.previous).previous).neLabel;
		   	else
		    	sense "-2" : ((NEWord)((NEWord)w.previous).previous).neTypeLevel1;
	 	}
	  }
  	}
}

real% prevTagsForContextLevel1(NEWord word) <-
{
	if(ParametersForLbjCode.featuresToUse.containsKey("prevTagsForContext"))
	{
	  	int i,j;
  		NEWord w = word;
		String[] words=new String[3];
		OccurrenceCounter[] count=new OccurrenceCounter[3];
	  	for (i = 0; i <= 2 && w != null; ++i) {
			count[i]=new OccurrenceCounter();
			words[i]=w.form;
			w = (NEWord) w.next;
		}
			
		w=(NEWord)word.previousIgnoreSentenceBoundary;
		for(i=0;i<1000&&w!=null;i++){
			for(j=0;j<words.length;j++){
				if(words[j]!=null&&w.form.equals(words[j])){
						if(NETaggerLevel1.isTraining/*&&ParametersForLbjCode.trainingIteration==0*/){
							if(ParametersForLbjCode.prevPredictionsLevel1RandomGenerator.useNoise())
								count[j].addToken(ParametersForLbjCode.prevPredictionsLevel1RandomGenerator.randomLabel());
							else					 
								count[j].addToken(w.neLabel);
						}
						else
			    			count[j].addToken(w.neTypeLevel1);
				}
			}
			w=(NEWord)w.previousIgnoreSentenceBoundary;
		}
	
		for(j=0;j<count.length;j++){
			if(count[j]!=null)	
			{
				String[] all=count[j].getTokens();
				for(i=0;i<all.length;i++)
					sense j+"_"+all[i] : count[j].getCount(all[i])/((double)count[j].totalTokens);	
			}
		}
	}
}

mixed% FeaturesLevel1SharedWithLevel2(NEWord word) <- nonLocalFeatures, GazetteersFeatures, FormParts ,Forms, Capitalization, WordTypeInformation, Affixes, BrownClusterPaths, WordEmbeddingFeatures, HmmEmbeddingFeatures, additionalFeaturesDiscreteNonConjunctive,additionalFeaturesDiscreteConjunctive, additionalFeaturesRealNonConjunctive 

mixed% FeaturesLevel1Only(NEWord word) <- PreviousTag1Level1,PreviousTag2Level1, prevTagsForContextLevel1, PreviousTag1Level1&&Forms, PreviousTag1Level1&&additionalFeaturesRealConjunctive, PreviousTag1Level1&& additionalFeaturesDiscreteConjunctive

discrete NETaggerLevel1(NEWord word)  <-
learn NELabel
  using FeaturesLevel1SharedWithLevel2,FeaturesLevel1Only
  with new SparseNetworkLearner(new SparseAveragedPerceptron(.1, 0, 16))
end



//---------------- CLASSIFIER LEVEL 2 -------------------


// Feature set iv
discrete% PreviousTag1Level2(NEWord word) <-
{
	if(ParametersForLbjCode.featuresToUse.containsKey("PreviousTag1"))
	{ 
	  	int i;
	  	NEWord w = word;
	  	if(w.previous!=null)
	  	{
			if (NETaggerLevel2.isTraining/*&&ParametersForLbjCode.trainingIteration==0*/) 
	    		sense "-1" : ((NEWord)w.previous).neLabel;
	    	else
	    		sense "-1" : ((NEWord)w.previous).neTypeLevel2;
      	}
    }
}

// Feature set iv
discrete% PreviousTag2Level2(NEWord word) <-
{
	if(ParametersForLbjCode.featuresToUse.containsKey("PreviousTag2"))
	{ 
	  int i;
	  NEWord w = word;
	  if(w.previous!=null)
	  {
		if(((NEWord)w.previous).previous!=null)
	  	{
			if (NETaggerLevel2.isTraining/*&&ParametersForLbjCode.trainingIteration==0*/) 
		   		sense "-2" : ((NEWord)((NEWord)w.previous).previous).neLabel;
		   	else
		    	sense "-2" : ((NEWord)((NEWord)w.previous).previous).neTypeLevel2;
	 	}
	  }
  	}
}

real% prevTagsForContextLevel2(NEWord word) <-
{
	if(ParametersForLbjCode.featuresToUse.containsKey("prevTagsForContext"))
	{
	  	int i,j;
  		NEWord w = word;
		String[] words=new String[3];
		OccurrenceCounter[] count=new OccurrenceCounter[3];
	  	for (i = 0; i <= 2 && w != null; ++i) {
			count[i]=new OccurrenceCounter();
			words[i]=w.form;
			w = (NEWord) w.next;
		}
			
		w=(NEWord)word.previousIgnoreSentenceBoundary;
		for(i=0;i<1000&&w!=null;i++){
			for(j=0;j<words.length;j++){
				if(words[j]!=null&&w.form.equals(words[j])){
						if(NETaggerLevel2.isTraining/*&&ParametersForLbjCode.trainingIteration==0*/) {
							if(ParametersForLbjCode.prevPredictionsLevel2RandomGenerator.useNoise())
								count[j].addToken(ParametersForLbjCode.prevPredictionsLevel2RandomGenerator.randomLabel());
							else					 
								count[j].addToken(w.neLabel);
						}
						else
			    			count[j].addToken(w.neTypeLevel2);
				}
			}
			w=(NEWord)w.previousIgnoreSentenceBoundary;
		}
	
		for(j=0;j<count.length;j++){
			if(count[j]!=null)	
			{
				String[] all=count[j].getTokens();
				for(i=0;i<all.length;i++)
					sense j+"_"+all[i] : count[j].getCount(all[i])/((double)count[j].totalTokens);	
			}
		}
	}
}



real% level1AggregationFeatures(NEWord word) <-
{
        if(ParametersForLbjCode.featuresToUse.containsKey("PredictionsLevel1")){

                int i=0;
                NEWord w = word, last = (NEWord)word.next;
  
                for (i = 0; i < 2 && last != null; ++i) last = (NEWord) last.next;
                for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;
  
                do 
                {
                                String[] arr=w.mostFrequentLevel1TokenInEntityType.getTokens();
                                for(int k=0;k<arr.length;k++)
                                        sense i+"1"+arr[k] : w.mostFrequentLevel1TokenInEntityType.getCount(arr[k])/w.mostFrequentLevel1TokenInEntityType.totalTokens;
                                arr=w.mostFrequentLevel1SuperEntityType.getTokens();
                                for(int k=0;k<arr.length;k++)
                                        sense i+"2"+arr[k] : w.mostFrequentLevel1SuperEntityType.getCount(arr[k])/w.mostFrequentLevel1SuperEntityType.totalTokens;
                                arr=w.mostFrequentLevel1ExactEntityType.getTokens();
                                for(int k=0;k<arr.length;k++)
                                        sense i+"3"+arr[k] : w.mostFrequentLevel1ExactEntityType.getCount(arr[k])/w.mostFrequentLevel1ExactEntityType.totalTokens;
                                arr=w.mostFrequentLevel1Prediction.getTokens();
                                for(int k=0;k<arr.length;k++)
                                        sense i+"4"+arr[k] : w.mostFrequentLevel1Prediction.getCount(arr[k])/w.mostFrequentLevel1Prediction.totalTokens;
                                arr=w.mostFrequentLevel1PredictionType.getTokens();
                                for(int k=0;k<arr.length;k++)
                                        sense i+"5"+arr[k] : w.mostFrequentLevel1PredictionType.getCount(arr[k])/w.mostFrequentLevel1PredictionType.totalTokens;
                                arr=w.mostFrequentLevel1NotOutsidePrediction.getTokens();
                                for(int k=0;k<arr.length;k++)
                                        sense i+"6"+arr[k] : w.mostFrequentLevel1NotOutsidePrediction.getCount(arr[k])/w.mostFrequentLevel1NotOutsidePrediction.totalTokens;
                                arr=w.mostFrequentLevel1NotOutsidePredictionType.getTokens();
                                for(int k=0;k<arr.length;k++)
                                        sense i+"7"+arr[k] : w.mostFrequentLevel1NotOutsidePredictionType.getCount(arr[k])/w.mostFrequentLevel1NotOutsidePredictionType.totalTokens;
                        i++;
                        w = (NEWord) w.next;
                }while(w != last);
        }
}

mixed% FeaturesLevel2(NEWord word) <-   level1AggregationFeatures,  PreviousTag1Level2, PreviousTag2Level2 ,  prevTagsForContextLevel2, PreviousTag1Level2&&Forms, PreviousTag1Level2&&additionalFeaturesDiscreteConjunctive, PreviousTag1Level2&&additionalFeaturesRealConjunctive
																	

discrete NETaggerLevel2(NEWord word)  <-
learn NELabel
  using FeaturesLevel1SharedWithLevel2,FeaturesLevel2
  with new SparseNetworkLearner(new SparseAveragedPerceptron(.1, 0, 20))
end
